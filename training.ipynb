{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817436a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "BASE_PATH = '/kaggle/input/wildlife-dataset'\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: 'Zebra', 1: 'Lion', 2: 'Leopard', 3: 'Cheetah', 4: 'Tiger',\n",
    "    5: 'Bear', 6: 'Butterfly', 7: 'Canary', 8: 'Crocodile', 9: 'Bull',\n",
    "    10: 'Camel', 11: 'Centipede', 12: 'Caterpillar', 13: 'Duck', 14: 'Squirrel',\n",
    "    15: 'Spider', 16: 'Ladybug', 17: 'Elephant', 18: 'Horse', 19: 'Fox',\n",
    "    20: 'Tortoise', 21: 'Frog', 22: 'Kangaroo', 23: 'Deer', 24: 'Eagle',\n",
    "    25: 'Monkey', 26: 'Snake', 27: 'Owl', 28: 'Swan', 29: 'Goat',\n",
    "    30: 'Rabbit', 31: 'Giraffe', 32: 'Goose', 33: 'PolarBear', 34: 'Raven',\n",
    "    35: 'Hippopotamus', 36: 'BrownBear', 37: 'Rhinoceros', 38: 'Woodpecker', 39: 'Sheep',\n",
    "    40: 'Magpie', 41: 'Ostrich', 42: 'Jaguar', 43: 'Hedgehog', 44: 'Turkey',\n",
    "    45: 'Raccoon', 46: 'Worm', 47: 'Harbor', 48: 'Panda', 49: 'RedPanda',\n",
    "    50: 'Otter', 51: 'Lynx', 52: 'Scorpion', 53: 'Koala'\n",
    "}\n",
    "\n",
    "# Optimized hyperparameters\n",
    "IMG_HEIGHT = 224  # Changed to 224 for better compatibility\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_CLASSES = 54\n",
    "\n",
    "USE_TRANSFER_LEARNING = True  # Set to False for pure AlexNet\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AlexNet Training - Best Practices Edition\")\n",
    "print(f\"Transfer Learning: {USE_TRANSFER_LEARNING}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def parse_yolo_label(label_path):\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            line = f.readline().strip()\n",
    "            if line:\n",
    "                return int(line.split()[0])\n",
    "    return None\n",
    "\n",
    "def load_dataset(split_name):\n",
    "    images_path = os.path.join(BASE_PATH, split_name, 'images')\n",
    "    labels_path = os.path.join(BASE_PATH, split_name, 'labels')\n",
    "\n",
    "    image_files = []\n",
    "    labels_list = []\n",
    "\n",
    "    if not os.path.exists(images_path):\n",
    "        return [], []\n",
    "\n",
    "    print(f\"Loading {split_name}...\")\n",
    "\n",
    "    for img_file in sorted(os.listdir(images_path)):\n",
    "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            label_path = os.path.join(labels_path, label_file)\n",
    "            class_id = parse_yolo_label(label_path)\n",
    "\n",
    "            if class_id is not None and 0 <= class_id < NUM_CLASSES:\n",
    "                image_files.append(img_path)\n",
    "                labels_list.append(class_id)\n",
    "\n",
    "    print(f\"  Loaded {len(image_files)} images\")\n",
    "    return image_files, labels_list\n",
    "\n",
    "# Load data\n",
    "train_images, train_labels = load_dataset('train')\n",
    "val_images, val_labels = load_dataset('valid')\n",
    "test_images, test_labels = load_dataset('test')\n",
    "\n",
    "print(f\"\\nTraining: {len(train_images)}, Validation: {len(val_images)}, Test: {len(test_images)}\")\n",
    "\n",
    "# Data generator\n",
    "class WildlifeDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, num_classes, augment=False):\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        if self.augment:\n",
    "            self.data_augmentation = keras.Sequential([\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                layers.RandomRotation(0.15),\n",
    "                layers.RandomZoom(0.15),\n",
    "                layers.RandomContrast(0.15),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i in batch_indices:\n",
    "            img = keras.utils.load_img(self.image_paths[i], target_size=self.img_size)\n",
    "            img_array = keras.utils.img_to_array(img) / 255.0\n",
    "            batch_images.append(img_array)\n",
    "            batch_labels.append(self.labels[i])\n",
    "\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = keras.utils.to_categorical(batch_labels, self.num_classes)\n",
    "\n",
    "        if self.augment:\n",
    "            batch_images = self.data_augmentation(batch_images, training=True)\n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.augment:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# Create AlexNet\n",
    "def create_alexnet(num_classes):\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(96, 11, strides=4, activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv2D(256, 5, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv2D(384, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Block 4\n",
    "        layers.Conv2D(384, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        # Block 5\n",
    "        layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(3, strides=2),\n",
    "\n",
    "        # Classifier\n",
    "        layers.GlobalAveragePooling2D(),  # Better than Flatten\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='AlexNet')\n",
    "    return model\n",
    "\n",
    "# Create transfer learning model\n",
    "def create_transfer_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='TransferLearning_ResNet50')\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Create generators\n",
    "train_gen = WildlifeDataGenerator(train_images, train_labels, BATCH_SIZE, (IMG_HEIGHT, IMG_WIDTH), NUM_CLASSES, augment=True)\n",
    "val_gen = WildlifeDataGenerator(val_images, val_labels, BATCH_SIZE, (IMG_HEIGHT, IMG_WIDTH), NUM_CLASSES, augment=False)\n",
    "test_gen = WildlifeDataGenerator(test_images, test_labels, BATCH_SIZE, (IMG_HEIGHT, IMG_WIDTH), NUM_CLASSES, augment=False)\n",
    "\n",
    "# Build model\n",
    "if USE_TRANSFER_LEARNING:\n",
    "    print(\"\\nBuilding Transfer Learning Model (ResNet50)...\")\n",
    "    model, base_model = create_transfer_model(NUM_CLASSES)\n",
    "else:\n",
    "    print(\"\\nBuilding AlexNet from scratch...\")\n",
    "    model = create_alexnet(NUM_CLASSES)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=8,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Phase 1: Train with frozen base (if transfer learning)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Phase 1: Training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS if not USE_TRANSFER_LEARNING else 30,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Phase 2: Fine-tune (if transfer learning)\n",
    "if USE_TRANSFER_LEARNING:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Phase 2: Fine-tuning (unfreezing layers)...\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Unfreeze the base model\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_acc')]\n",
    "    )\n",
    "\n",
    "    history2 = model.fit(\n",
    "        train_gen,\n",
    "        epochs=30,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Combine histories\n",
    "    for key in history1.history.keys():\n",
    "        history1.history[key].extend(history2.history[key])\n",
    "\n",
    "history = history1\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Final Evaluation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = model.evaluate(test_gen, verbose=1)\n",
    "print(f\"\\nTest Accuracy: {test_results[1]*100:.2f}%\")\n",
    "print(f\"Test Top-5 Accuracy: {test_results[2]*100:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 0].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 1].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history.history['top5_acc'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_top5_acc'], label='Val', linewidth=2)\n",
    "axes[1, 0].set_title('Top-5 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "gap = np.array(history.history['accuracy']) - np.array(history.history['val_accuracy'])\n",
    "axes[1, 1].plot(gap, linewidth=2, color='red')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Overfitting Gap', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_training_history.png', dpi=300)\n",
    "print(\"Plot saved!\")\n",
    "plt.show()\n",
    "\n",
    "model.save('final_wildlife_model.h5')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(CLASS_NAMES, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
